{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install portalocker, torchdata (pip), torchtext (pip)\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "bptt = 35 \t\t\t\t# max sentence length\n",
    "emsize = 200\t\t\t# embedding dim\n",
    "d_hid = 200\t\t\t\t# dim of feedfwd in TransformerEncoder\n",
    "nlayers = 2 \t\t\t# number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = 2 \t\t\t\t# number of heads in MultiheadAttention\n",
    "dropout = 0.2 \t\t\t# dropout probability\n",
    "lr = 5.0 \t\t\t\t# learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "\t# convert raw text into a flat tensor\n",
    "\tdata = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "\treturn torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "\t# divide data into bsz separate sequences\n",
    "\tseq_len = data.size(0) // bsz\n",
    "\tdata = data[:seq_len * bsz]\n",
    "\tdata = data.view(bsz, seq_len).t().contiguous()\n",
    "\treturn data.to(device)\n",
    "\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "\tseq_len = min(bptt, len(source) - 1 - i)\n",
    "\tdata = source[i:i+seq_len]\n",
    "\ttarget = source[i+1:i+1+seq_len].reshape(-1)\n",
    "\treturn data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\tdef __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "\t\tnlayers:int, dropout: float = 0.5):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model_type = 'Transformer'\n",
    "\t\tself.d_model = d_model\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(ntoken, d_model)\n",
    "\t\tself.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\t\t\n",
    "\t\tencoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\t\tself.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "\t\tself.init_weights()\n",
    "\n",
    "\tdef init_weights(self) -> None:\n",
    "\t\tinitrange = 0.1\n",
    "\t\tself.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.linear.bias.data.zero_()\n",
    "\t\tself.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\tdef forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "\t\t# src shape [seq_len, batch_size], src mask shape [seq_len, seq_len], out shape [seq_len, batch_size, ntoken]\n",
    "\t\tsrc = self.embedding(src) * math.sqrt(self.d_model)\n",
    "\t\tsrc = self.pos_encoder(src)\n",
    "\t\toutput = self.transformer_encoder(src, src_mask)\n",
    "\t\toutput = self.linear(output)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\tdef __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "\t\tposition = torch.arange(max_len).unsqueeze(1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\t\tpe = torch.zeros(max_len, 1, d_model)\n",
    "\t\tpe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "\t\tself.register_buffer('pe', pe)\t\t\t# pe is not trained, but saved with model\n",
    "\n",
    "\tdef forward(self, x: Tensor) -> Tensor:\n",
    "\t\tx = x + self.pe[:x.size(0)]\n",
    "\t\treturn self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "\tmodel.train()\n",
    "\n",
    "\ttotal_loss, log_interval = 0., 200\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tnum_batches = len(train_data) // bptt\n",
    "\tfor batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "\t\tdata, targets = get_batch(train_data, i)\n",
    "\t\toutput = model(data)\n",
    "\t\toutput_flat = output.view(-1, ntokens)\n",
    "\t\tloss = criterion(output_flat, targets)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttotal_loss += loss.item()\n",
    "\t\tif batch % log_interval == 0 and batch > 0:\n",
    "\t\t\tlr = scheduler.get_last_lr()[0]\n",
    "\t\t\tms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "\t\t\tcur_loss = total_loss / log_interval\n",
    "\t\t\tppl = math.exp(cur_loss)\n",
    "\t\t\tprint(f'| epoch {epoch:3d} | {batch:5d}|{num_batches:5d} batches |'\n",
    "\t\t\t\t  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "\t\t\t\t  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "\t\t\ttotal_loss = 0\n",
    "\t\t\tstart_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "\tmodel.eval()  # turn on evaluation mode\n",
    "\ttotal_loss = 0.\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i in range(0, eval_data.size(0) - 1, bptt):\n",
    "\t\t\tdata, targets = get_batch(eval_data, i)\n",
    "\t\t\tseq_len = data.size(0)\n",
    "\t\t\toutput = model(data)\n",
    "\t\t\toutput_flat = output.view(-1, ntokens)\n",
    "\t\t\ttotal_loss += seq_len * criterion(output_flat, targets).item()\n",
    "\treturn total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tamar\\Desktop\\תואר\\Samester D\\למידה עמוקה יישומית\\תכנות - כיתה\\lecture_11.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \tepoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \ttrain(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \tval_loss \u001b[39m=\u001b[39m evaluate(model, val_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \tval_ppl \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mexp(val_loss)\n",
      "\u001b[1;32mc:\\Users\\tamar\\Desktop\\תואר\\Samester D\\למידה עמוקה יישומית\\תכנות - כיתה\\lecture_11.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m output_flat \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ntokens)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output_flat, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "\tbest_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\tepoch_start_time = time.time()\n",
    "\t\ttrain(model)\n",
    "\t\tval_loss = evaluate(model, val_data)\n",
    "\t\tval_ppl = math.exp(val_loss)\n",
    "\t\telapsed = time.time() - epoch_start_time\n",
    "\t\tprint('-' * 89)\n",
    "\t\tprint(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "\t\t\tf'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "\t\tprint('-' * 89)\n",
    "\n",
    "\t\tif val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = val_loss\n",
    "\t\t\ttorch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "\t\tscheduler.step()\n",
    "\tmodel.load_state_dict(torch.load(best_model_params_path)) # load best model states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tamar\\Desktop\\תואר\\Samester D\\למידה עמוקה יישומית\\תכנות - כיתה\\lecture_11.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_loss \u001b[39m=\u001b[39m evaluate(model, test_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_ppl \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mexp(test_loss)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tamar/Desktop/%D7%AA%D7%95%D7%90%D7%A8/Samester%20D/%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94%20%D7%99%D7%99%D7%A9%D7%95%D7%9E%D7%99%D7%AA/%D7%AA%D7%9B%D7%A0%D7%95%D7%AA%20-%20%D7%9B%D7%99%D7%AA%D7%94/lecture_11.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m89\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "\tf'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
