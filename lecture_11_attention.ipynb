{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "path = './'\n",
    "resume_train = False\n",
    "MAX_LENGTH = 10 \t# max length of sentence\n",
    "tfr = 0.5 \t\t\t# teacher_forcing_ratio\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def asMinutes(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "\tnow = time.time()\n",
    "\ts = now - since\n",
    "\tes = s / percent\n",
    "\trs = es - s\n",
    "\treturn '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "\tfig, ax = plt.subplots()\n",
    "\t# this locator puts ticks at regular intervals\n",
    "\tloc = ticker.MultipleLocator(base=0.2)\n",
    "\tax.yaxis.set_major_locator(loc)\n",
    "\tplt.plot(points)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "\tdef __init__(self, name):\n",
    "\t\tself.name = name\n",
    "\t\tself.word2index = {}\n",
    "\t\tself.word2count = {}\n",
    "\t\tself.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "\t\tself.n_words = 2 \t# Count SOS and EOS\n",
    "\n",
    "\tdef addSentence(self, sentence):\n",
    "\t\tfor word in sentence.split(' '):\n",
    "\t\t\tself.addWord(word)\n",
    "\n",
    "\tdef addWord(self, word):\n",
    "\t\tif word not in self.word2index:\n",
    "\t\t\tself.word2index[word] = self.n_words\n",
    "\t\t\tself.word2count[word] = 1\n",
    "\t\t\tself.index2word[self.n_words] = word\n",
    "\t\t\tself.n_words += 1\n",
    "\t\telse:\n",
    "\t\t\tself.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "\tprint(\"Reading lines...\")\n",
    "\n",
    "\t# Read the file and split into lines\n",
    "\tlines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "\t\tread().strip().split('\\n')\n",
    "\n",
    "\t# Split every line into pairs and normalize\n",
    "\tpairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "\t# Reverse pairs, make Lang instances\n",
    "\tif reverse:\n",
    "\t\tpairs = [list(reversed(p)) for p in pairs]\n",
    "\t\tinput_lang = Lang(lang2)\n",
    "\t\toutput_lang = Lang(lang1)\n",
    "\telse:\n",
    "\t\tinput_lang = Lang(lang1)\n",
    "\t\toutput_lang = Lang(lang2)\n",
    "\n",
    "\treturn input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the dataset to sentence with max. 10 words\n",
    "eng_prefixes = (\n",
    "\t\"i am \", \"i m \",\n",
    "\t\"he is\", \"he s \",\n",
    "\t\"she is\", \"she s \",\n",
    "\t\"you are\", \"you re \",\n",
    "\t\"we are\", \"we re \",\n",
    "\t\"they are\",\"they re \")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "\treturn len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "\t\tlen(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "\t\tp[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "\treturn [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "\tinput_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "\tprint('Read %s sentence pairs' % len(pairs))\n",
    "\tpairs = filterPairs(pairs)\n",
    "\tprint('Trimmed to %s sentence pairs' % len(pairs))\n",
    "\tprint('Counting words...')\n",
    "\tfor pair in pairs:\n",
    "\t\tinput_lang.addSentence(pair[0])\n",
    "\t\toutput_lang.addSentence(pair[1])\n",
    "\tprint('Counted words:')\n",
    "\tprint(input_lang.name, input_lang.n_words)\n",
    "\tprint(output_lang.name, output_lang.n_words)\n",
    "\treturn input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def indicesFromSentence(lang, sentence):\n",
    "\treturn [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "\tindices = indicesFromSentence(lang, sentence)\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "\tinput_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "\ttarget_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "\treturn (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EncoderRNN(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size):\n",
    "\t\tsuper(EncoderRNN, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\t\tself.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "\tdef forward(self, input, hidden):\n",
    "\t\tembedded = self.embedding(input).view(1, 1, -1)\n",
    "\t\toutput = embedded\n",
    "\t\toutput, hidden = self.gru(output, hidden)\n",
    "\t\treturn output, hidden\n",
    "\n",
    "\tdef initHidden(self):\n",
    "\t\treturn torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\tdef __init__(self, hidden_size, output_size):\n",
    "\t\tsuper(DecoderRNN, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\t\tself.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\t\tself.out = nn.Linear(hidden_size, output_size)\n",
    "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, input, hidden):\n",
    "\t\toutput = self.embedding(input).view(1, 1, -1)\n",
    "\t\toutput = self.relu(output)\n",
    "\t\toutput, hidden = self.gru(output, hidden)\n",
    "\t\toutput = self.softmax(self.out(output[0]))\n",
    "\t\treturn output, hidden\n",
    "\n",
    "\tdef initHidden(self):\n",
    "\t\treturn torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Decoder\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "\tdef __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "\t\tsuper(AttnDecoderRNN, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.dropout_p = dropout_p\n",
    "\t\tself.max_length = max_length\n",
    "\t\tself.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\t\tself.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\t\tself.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\t\tself.alignment_vector = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\ttorch.nn.init.xavier_uniform_(self.alignment_vector)\n",
    "\t\tself.dropout = nn.Dropout(self.dropout_p)\n",
    "\t\tself.gru = nn.GRU(self.hidden_size * 2, self.hidden_size)\n",
    "\t\tself.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
    "\t\n",
    "\tdef forward(self, input, hidden, encoder_outputs):\n",
    "\t\tembedded = self.embedding(input).view(1, -1)\n",
    "\t\tembedded = self.dropout(embedded)\n",
    "\t\ttransformed_hidden = self.fc_hidden(hidden[0])\n",
    "\t\texpanded_hidden_state = transformed_hidden.expand(self.max_length, -1)\n",
    "\t\talignment_scores = torch.tanh(expanded_hidden_state + \n",
    "\t\t\tself.fc_encoder(encoder_outputs))\n",
    "\t\talignment_scores = self.alignment_vector.mm(alignment_scores.T)\n",
    "\t\tattn_weights = self.softmax(alignment_scores)\n",
    "\t\tcontext_vector = attn_weights.mm(encoder_outputs)\n",
    "\t\toutput = torch.cat((embedded, context_vector), 1).unsqueeze(0)\n",
    "\t\toutput, hidden = self.gru(output, hidden)\n",
    "\t\toutput = self.softmax(self.out(output[0]))\n",
    "\t\treturn output, hidden, attn_weights\n",
    "\t\n",
    "\tdef initHidden(self):\n",
    "\t\treturn torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, output_size):\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\n",
    "\t\tself.max_length = MAX_LENGTH\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.enc = EncoderRNN(input_size, hidden_size)\n",
    "\t\t# self.dec = DecoderRNN(hidden_size, output_size)\n",
    "\t\tself.dec = AttnDecoderRNN(hidden_size, output_size)\n",
    "\n",
    "\tdef forward(self, src_tensor, tar_tensor, criterion):\n",
    "\t\t# src_tensor n_src_words x 1, tar_tensor n_tar_words\n",
    "\t\tenc_hidden = self.enc.initHidden()\n",
    "\n",
    "\t\t# encoder loop; ignore encoder output\n",
    "\t\tenc_outputs = torch.zeros(self.max_length, self.enc.hidden_size, device=src_tensor.device)\n",
    "\t\tfor ei in range(src_tensor.size(0)):\n",
    "\t\t\tenc_output, enc_hidden = self.enc(src_tensor[ei], enc_hidden)\n",
    "\t\t\tenc_outputs[ei] = enc_output[0, 0]\n",
    "\n",
    "\t\tbatch_size = enc_outputs.size(0)\n",
    "\t\tdec_input = torch.tensor([[SOS_token]], device=device)\n",
    "\t\tdec_hidden = enc_hidden\t\t# instead of using initHidden()\n",
    "\n",
    "\t\tloss = 0\n",
    "\n",
    "\t\tif random.random() < tfr:\n",
    "\t\t\t# Teacher forcing: feed the target as the next input\n",
    "\t\t\tfor di in range(tar_tensor.size(0)):\n",
    "\t\t\t\tdec_output, dec_hidden, attn_weights = self.dec(dec_input, \n",
    "\t\t\t\t\tdec_hidden, enc_outputs)\n",
    "\n",
    "\t\t\t\tloss += criterion(dec_output, tar_tensor[di])\n",
    "\t\t\t\tdec_input = tar_tensor[di]\t# Teacher forcing\n",
    "\t\telse:\n",
    "\t\t\t# without Teacher forcing: use its own predictions as the next input\n",
    "\t\t\tfor di in range(tar_tensor.size(0)):\n",
    "\t\t\t\tdec_output, dec_hidden, attn_weights = self.dec(dec_input, \n",
    "\t\t\t\t\tdec_hidden, enc_outputs)\n",
    "\n",
    "\t\t\t\ttopv, topi = dec_output.topk(1)\n",
    "\t\t\t\tdec_input = topi.squeeze().detach() # detach from history as input\n",
    "\n",
    "\t\t\t\tloss += criterion(dec_output, tar_tensor[di])\n",
    "\t\t\t\tif dec_input.item() == EOS_token:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\treturn loss\n",
    "\tdef evaluate(self, sentence):\n",
    "\t\tsrc_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "\t\t# encoder\n",
    "\t\tenc_hidden = self.enc.initHidden()\n",
    "\t\tenc_outputs = torch.zeros(self.max_length, self.enc.hidden_size, device=src_tensor.device)\n",
    "\t\tfor ei in range(src_tensor.size(0)):\n",
    "\t\t\tenc_output, enc_hidden = self.enc(src_tensor[ei], enc_hidden)\n",
    "\t\t\tenc_outputs[ei] += enc_output[0, 0]\n",
    "\n",
    "\t\t# decoder\n",
    "\t\tdec_input = torch.tensor([[SOS_token]], device=device)\n",
    "\t\tdec_hidden = enc_hidden\n",
    "\n",
    "\t\tdecoded_words, dec_attns = [], torch.zeros(self.max_length, self.max_length)\n",
    "\t\tfor di in range(self.max_length):\n",
    "\t\t\tdec_output, dec_hidden, dec_attn = self.dec(dec_input, dec_hidden, enc_outputs)\n",
    "\n",
    "\t\t\tdec_attns[di] = dec_attn.data\n",
    "\n",
    "\t\t\t_, topi = dec_output.topk(1)\n",
    "\t\t\tif topi.item() == EOS_token:\n",
    "\t\t\t\tdecoded_words.append('<EOS>')\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "\t\t\tdec_input = topi.squeeze().detach()\n",
    "\n",
    "\t\treturn decoded_words, dec_attns[:di+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(model, optimizer, criterion, n_iters, print_every=1000, plot_every=100):\n",
    "\tstart = time.time()\n",
    "\n",
    "\t# create batch\n",
    "\ttraining_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "\tmodel.train()\n",
    "\tprint_loss_total, plot_loss_total, plot_losses = 0, 0, []\n",
    "\tfor iter in range(1, n_iters + 1):\n",
    "\t\ttraining_pair = training_pairs[iter - 1]\n",
    "\t\tsrc_tensor, tar_tensor = training_pair[0], training_pair[1]\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss = model(src_tensor.to(device), tar_tensor.to(device), criterion)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tprint_loss_total += loss.item() / tar_tensor.size(0)\n",
    "\t\tplot_loss_total += loss.item() / tar_tensor.size(0)\n",
    "\n",
    "\t\tif iter % print_every == 0:\n",
    "\t\t\tprint_loss_avg = print_loss_total / print_every\n",
    "\t\t\tprint_loss_total = 0\n",
    "\t\t\tprint('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), \n",
    "\t\t\t\titer, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "\t\t\ttorch.save(model.state_dict(), path + 'model.pt')\n",
    "\n",
    "\t\tif iter % plot_every == 0:\n",
    "\t\t\tplot_loss_avg = plot_loss_total / plot_every\n",
    "\t\t\tplot_losses.append(plot_loss_avg)\n",
    "\t\t\tplot_loss_total = 0\n",
    "\n",
    "\tshowPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['nous en sommes toutes responsables .', 'we re all to blame for that .']\n",
      "0m 3s (- 0m 34s) (100 10%) 4.0153\n",
      "0m 7s (- 0m 30s) (200 20%) 3.4452\n",
      "0m 11s (- 0m 27s) (300 30%) 3.5593\n",
      "0m 16s (- 0m 24s) (400 40%) 3.4383\n",
      "0m 20s (- 0m 20s) (500 50%) 3.1240\n",
      "0m 25s (- 0m 16s) (600 60%) 3.2889\n",
      "0m 29s (- 0m 12s) (700 70%) 3.1641\n",
      "0m 33s (- 0m 8s) (800 80%) 3.1362\n",
      "0m 37s (- 0m 4s) (900 90%) 2.8617\n",
      "0m 42s (- 0m 0s) (1000 100%) 3.2464\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "def evaluate(model, n=10):\n",
    "\tmodel.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor ii in range(n):\n",
    "\t\t\tpair = random.choice(pairs)\n",
    "\t\t\tprint('>', pair[0])\n",
    "\t\t\tprint('=', pair[1])\n",
    "\t\t\toutput_words, attns = model.evaluate(pair[0])\n",
    "\t\t\toutput_sentence = ' '.join(output_words)\n",
    "\t\t\tprint('<', output_sentence)\n",
    "\t\t\tprint('')\n",
    "\n",
    "\tplt.imshow(attns)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "# pre-process data, print a sample\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))\n",
    "\n",
    "\n",
    "# create model, optimizer, and criterion\n",
    "model = Model(input_lang.n_words, hidden_size, output_lang.n_words).to(device)\n",
    "resume_train = False\n",
    "if resume_train:\n",
    "\tmodel.load_state_dict(torch.load(path + 'model.pt'))\n",
    "\n",
    "\t\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# train model\n",
    "train(model, optimizer, criterion, 1000, print_every=100, plot_every=100)\n",
    "\n",
    "# evaluate model\n",
    "evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
